# data_analysis
Jupyter notebooks as my way of learning data science


!! IMPORTANT NOTE !! - I use JS in my notebooks which does not work here, if you want to read through some, please load the specific notebook in nbviewer:
https://nbviewer.jupyter.org/

I create these notebooks as "guides" to teach absolute basics of data science. As I am learning each concept I write these notebooks and test them at the same time, thus there will most likely be a lot of mistakes, bad practices and and inefficient code. But, hands on practice is the best way to learn something, so here I am ! Any constructive criticism is more than welcome, as I will get more hands on experience, knowledge, tips and suggestions from the community, I will come back and modify these notebooks.

All datasets I used are open databases from kaggle.com and I would like to give a big thanks to the kagle.com and all the community that makes it much easier to start with this 
exciting field !



Actually the inteded sections are like this:


1)	Preprocessing
-	I want to cover tha basics you will need from loading the dataset to the part where it is clean, well-arranged and uniformed, simply put, in top notch state for modeling or any other purpose

  -	Load, understand and explore you dataset
  -	Find and decide how to solve blank cells and outliers
  -	Clean and uniform the dataset
  -	Feature engineering â€“ according to the task, create/delete/adjust columns



2)	Ploting

-	More indepth look into ploting possibilites. We will make some ploting in each of these notebooks needed for the specific part, but here I would like to concetrate some more advanced techniques and visual options. I will focus on Seaborn library (which is based on matplotlib) as it has some nice utility and is really easy to start with (at least for me, I find it much more intuitive than plain matplotlib to be honest)



3)	Machine learning algos

-	I will do Jupyter Notebook for each type of ML algo I will work with, starting with the basics and progress to more complex ones. I will try to provide already clean datasets or we will use datasets that are by default ready to go.



4)	Model situations

-	Finally, we will put to the test all the knowledge we learned to do some real model situations (artificially tho). You will get the dataset and an assignment what to do... And that will be all, now do the magic! I will try to find some insteresting datasets and diverse tasks, so it would be beneficial and not repetitive. For example, doing some predictions, extract me those X informations, prepare a presentation for stakeholders, find ways we can save some money etc.
